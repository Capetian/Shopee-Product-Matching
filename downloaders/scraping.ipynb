{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from playwright.async_api import async_playwright\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import aiohttp        \n",
    "import aiofiles\n",
    "import time\n",
    "import os.path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECTING WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data'\n",
    "# base_path = 'data'\n",
    "train_df = pd.read_csv(base_path + '/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>train_4028265689</td>\n",
       "      <td>fff1c07ceefc2c970a7964cfb81981c5.jpg</td>\n",
       "      <td>e3cd72389f248f21</td>\n",
       "      <td>Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...</td>\n",
       "      <td>3776555725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34246</th>\n",
       "      <td>train_769054909</td>\n",
       "      <td>fff401691371bdcb382a0d9075dfea6a.jpg</td>\n",
       "      <td>be86851f72e2853c</td>\n",
       "      <td>MamyPoko Pants Royal Soft - S 70 - Popok Celana</td>\n",
       "      <td>2736479533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n",
       "      <td>ad27f0d08c0fcbf0</td>\n",
       "      <td>KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...</td>\n",
       "      <td>4101248785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34248</th>\n",
       "      <td>train_3630949769</td>\n",
       "      <td>fff51b87916dbfb6d0f8faa01bee67b8.jpg</td>\n",
       "      <td>e3b13bd1d896c05c</td>\n",
       "      <td>Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...</td>\n",
       "      <td>1663538013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>train_1792180725</td>\n",
       "      <td>ffffa0ab2ae542357671e96254fa7167.jpg</td>\n",
       "      <td>af8bc4b2d2cf9083</td>\n",
       "      <td>FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...</td>\n",
       "      <td>459464107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image  \\\n",
       "0       train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg   \n",
       "1      train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2      train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4      train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n",
       "...                 ...                                   ...   \n",
       "34245  train_4028265689  fff1c07ceefc2c970a7964cfb81981c5.jpg   \n",
       "34246   train_769054909  fff401691371bdcb382a0d9075dfea6a.jpg   \n",
       "34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n",
       "34248  train_3630949769  fff51b87916dbfb6d0f8faa01bee67b8.jpg   \n",
       "34249  train_1792180725  ffffa0ab2ae542357671e96254fa7167.jpg   \n",
       "\n",
       "            image_phash                                              title  \\\n",
       "0      94974f937d4c2433                          Paper Bag Victoria Secret   \n",
       "1      af3f9460c2838f0f  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   \n",
       "2      b94cb00ed3e50f78        Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3      8514fc58eafea283  Daster Batik Lengan pendek - Motif Acak / Camp...   \n",
       "4      a6f319f924ad708c                  Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "...                 ...                                                ...   \n",
       "34245  e3cd72389f248f21  Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...   \n",
       "34246  be86851f72e2853c    MamyPoko Pants Royal Soft - S 70 - Popok Celana   \n",
       "34247  ad27f0d08c0fcbf0  KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...   \n",
       "34248  e3b13bd1d896c05c  Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...   \n",
       "34249  af8bc4b2d2cf9083  FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...   \n",
       "\n",
       "       label_group  \n",
       "0        249114794  \n",
       "1       2937985045  \n",
       "2       2395904891  \n",
       "3       4093212188  \n",
       "4       3648931069  \n",
       "...            ...  \n",
       "34245   3776555725  \n",
       "34246   2736479533  \n",
       "34247   4101248785  \n",
       "34248   1663538013  \n",
       "34249    459464107  \n",
       "\n",
       "[34250 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge5 = train_df.groupby(['label_group']).filter(lambda x: x['title'].size >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_1802986387</td>\n",
       "      <td>00144a49c56599d45354a1c28104c039.jpg</td>\n",
       "      <td>f815c9bb833ab4c8</td>\n",
       "      <td>Jubah anak size 1-12 thn</td>\n",
       "      <td>1835033137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_4196427721</td>\n",
       "      <td>002039aaf8618627a0442d5e89e5dda6.jpg</td>\n",
       "      <td>e98c873acc65946e</td>\n",
       "      <td>Korek Kuping LED untuk balita CherryBabyKidsSh...</td>\n",
       "      <td>349297863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>train_2961381387</td>\n",
       "      <td>00303ad1c062fdeaf5f41b9ffb71a5fb.jpg</td>\n",
       "      <td>e48d9b652098efe1</td>\n",
       "      <td>Madame Gie MakeUp Blush On BY GISELL</td>\n",
       "      <td>2098400894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>train_1396146683</td>\n",
       "      <td>0035b8fc42955c0fa2301b68e2ffe861.jpg</td>\n",
       "      <td>d5c0caa0aeb3b23e</td>\n",
       "      <td>CHOCO BALL LAGIE COKLAT Lagie Grosir Cokelat L...</td>\n",
       "      <td>4240123481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>train_1872347548</td>\n",
       "      <td>003906c6c6bba4cde15e5e854dba8f6d.jpg</td>\n",
       "      <td>f3e18c5ea14493e3</td>\n",
       "      <td>Stand Hanger Multifungsi - Tiang Gantungan Baj...</td>\n",
       "      <td>4044205497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34210</th>\n",
       "      <td>train_3738631799</td>\n",
       "      <td>ffa7e451b9a6f15060351316c731d057.jpg</td>\n",
       "      <td>ea9dc137c24a8e63</td>\n",
       "      <td>b\"DRESS MUSLIM MURAH HAYLA DRESS '\"</td>\n",
       "      <td>3861748824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34217</th>\n",
       "      <td>train_2051967800</td>\n",
       "      <td>ffb88cdb27676ed9f142a6ee615b6452.jpg</td>\n",
       "      <td>e9dccb5c421f1638</td>\n",
       "      <td>Lampu Tumblr agustusan warna warni bisa disamb...</td>\n",
       "      <td>1018873174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34218</th>\n",
       "      <td>train_3349791994</td>\n",
       "      <td>ffb9b59c7669c781847fba7499bd7699.jpg</td>\n",
       "      <td>f4ce05c51f71705a</td>\n",
       "      <td>LACHEL VITAMIN C LACHEL VIT C GLUTA FROZEN COL...</td>\n",
       "      <td>1911010635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34235</th>\n",
       "      <td>train_3668806308</td>\n",
       "      <td>ffdd054398884b04bdab9e41d30c7fb2.jpg</td>\n",
       "      <td>9ba6cc38a4a792f1</td>\n",
       "      <td>MAYCREATE MOISTURIZING SPRAY 150ML(1KG 6PCS)  ...</td>\n",
       "      <td>1141798720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n",
       "      <td>ad27f0d08c0fcbf0</td>\n",
       "      <td>KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...</td>\n",
       "      <td>4101248785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11507 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image  \\\n",
       "6      train_1802986387  00144a49c56599d45354a1c28104c039.jpg   \n",
       "15     train_4196427721  002039aaf8618627a0442d5e89e5dda6.jpg   \n",
       "19     train_2961381387  00303ad1c062fdeaf5f41b9ffb71a5fb.jpg   \n",
       "22     train_1396146683  0035b8fc42955c0fa2301b68e2ffe861.jpg   \n",
       "24     train_1872347548  003906c6c6bba4cde15e5e854dba8f6d.jpg   \n",
       "...                 ...                                   ...   \n",
       "34210  train_3738631799  ffa7e451b9a6f15060351316c731d057.jpg   \n",
       "34217  train_2051967800  ffb88cdb27676ed9f142a6ee615b6452.jpg   \n",
       "34218  train_3349791994  ffb9b59c7669c781847fba7499bd7699.jpg   \n",
       "34235  train_3668806308  ffdd054398884b04bdab9e41d30c7fb2.jpg   \n",
       "34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n",
       "\n",
       "            image_phash                                              title  \\\n",
       "6      f815c9bb833ab4c8                           Jubah anak size 1-12 thn   \n",
       "15     e98c873acc65946e  Korek Kuping LED untuk balita CherryBabyKidsSh...   \n",
       "19     e48d9b652098efe1               Madame Gie MakeUp Blush On BY GISELL   \n",
       "22     d5c0caa0aeb3b23e  CHOCO BALL LAGIE COKLAT Lagie Grosir Cokelat L...   \n",
       "24     f3e18c5ea14493e3  Stand Hanger Multifungsi - Tiang Gantungan Baj...   \n",
       "...                 ...                                                ...   \n",
       "34210  ea9dc137c24a8e63                b\"DRESS MUSLIM MURAH HAYLA DRESS '\"   \n",
       "34217  e9dccb5c421f1638  Lampu Tumblr agustusan warna warni bisa disamb...   \n",
       "34218  f4ce05c51f71705a  LACHEL VITAMIN C LACHEL VIT C GLUTA FROZEN COL...   \n",
       "34235  9ba6cc38a4a792f1  MAYCREATE MOISTURIZING SPRAY 150ML(1KG 6PCS)  ...   \n",
       "34247  ad27f0d08c0fcbf0  KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...   \n",
       "\n",
       "       label_group  \n",
       "6       1835033137  \n",
       "15       349297863  \n",
       "19      2098400894  \n",
       "22      4240123481  \n",
       "24      4044205497  \n",
       "...            ...  \n",
       "34210   3861748824  \n",
       "34217   1018873174  \n",
       "34218   1911010635  \n",
       "34235   1141798720  \n",
       "34247   4101248785  \n",
       "\n",
       "[11507 rows x 5 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = train_df.groupby(['label_group']).filter(lambda x: x['title'].size < 6)\n",
    "labels = groups['label_group'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups['title'] = [ s.encode('latin1').decode('unicode-escape').encode('latin1').decode('utf-8') for s in groups['title']]\n",
    "groups['title'] = groups.title.str.lower()\n",
    "groups['title'] = [re.sub('[^0-9a-zA-Z ,.]+', ' ', x) for x in groups['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups['len_words'] = [len(re.sub('[^0-9a-zA-Z ,.]+', ' ', x).split()) for x in groups['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = groups.groupby('label_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "for id in labels:\n",
    "    product = {}\n",
    "    curr = grouped.get_group(id)\n",
    "    product['label_group'] = id\n",
    "    base = r'^{}'\n",
    "    expr = '(?=.*{})'\n",
    "    v = TfidfVectorizer()\n",
    "    x = v.fit_transform(curr['title'])\n",
    "    y = v.transform([' '.join(curr[\"title\"])])\n",
    "    feature_array = v.get_feature_names_out()\n",
    "    if feature_array.shape[0] > 5:\n",
    "        tfidf_sorting = np.argsort(y.toarray()).flatten()[::-1]\n",
    "        top_n = feature_array[tfidf_sorting][:5].tolist()\n",
    "        product['query'] = ('https://shopee.co.id/search?keyword=' + \" \".join(top_n)).replace(\" \", \"%20\")\n",
    "        \n",
    "        product['words'] = base.format(''.join(expr.format(w) for w in top_n))\n",
    "    else:\n",
    "        product['query'] = ('https://shopee.co.id/search?keyword=' + curr['title'].iloc[0] ).replace(\" \", \"%20\") \n",
    "       \n",
    "        product['words'] =  base.format(''.join(expr.format(w) for w in feature_array.tolist()))\n",
    "    products.append(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_img(url,title,label_group):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as resp:\n",
    "            if resp.status == 200:\n",
    "                # start = time.time()\n",
    "                f = await aiofiles.open(title, mode='wb+')\n",
    "                await f.write(await resp.read())\n",
    "                await f.close()\n",
    "                # end = time.time()\n",
    "                # print(f\"{title} at {label_group} downloaded for {end-start} secs\")\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "\n",
    "async def get_info(item,label_group,words):\n",
    "    res = dict()\n",
    "    title = await item.get_attribute('alt')\n",
    "    matched = (re.sub('[^0-9a-zA-Z ,]+', ' ', title)).lower()\n",
    "    if re.match(words, matched):\n",
    "        link = await item.get_attribute('src')\n",
    "        url = link[:-3]\n",
    "        name = re.sub('[^0-9a-zA-Z ,]+', ' ', title)\n",
    "        name = \" \".join(name.split()[:6])\n",
    "        name = base_path + '/scrap_image/' + name + \"_\" + str(np.random.randint(0, 60)) + \"_\" + str(label_group)  + '.jpg'\n",
    "        name = name.replace(' ', \"_\")\n",
    "        down = await download_img(url,name,label_group)\n",
    "        if down:\n",
    "            res['url'] = url\n",
    "            res['image'] = name\n",
    "            res['title'] = title\n",
    "            res['label_group'] = label_group\n",
    "            return res\n",
    "        else:\n",
    "            res['url'] = url\n",
    "            res['image'] = name\n",
    "            res['title'] = title\n",
    "            res['label_group'] = label_group\n",
    "            return (res)\n",
    "\n",
    "async def scrape_category(product,timeout=30000):\n",
    "    async with async_playwright() as pw:\n",
    "        # start = time.time()\n",
    "        browser = await pw.chromium.launch( headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(product['query'])\n",
    "        # start_wait = time.time()\n",
    "        res = await page.wait_for_selector('.shopee-search-item-result', timeout=timeout)\n",
    "        end_wait = time.time()\n",
    "        # print(f\"product {product['label_group']} waiting page for {end_wait-start_wait} secs\")\n",
    "        for i in range(3):\n",
    "            await page.wait_for_selector('._7DTxhh.vc8g9F')\n",
    "            await page.keyboard.down('End')\n",
    "        selector = '._7DTxhh.vc8g9F'\n",
    "        all_items = await res.query_selector_all(selector)\n",
    "        tasks = []\n",
    "        for item in all_items:\n",
    "            tasks.append(asyncio.create_task(get_info(item,product['label_group'],product['words'])))\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        await browser.close()\n",
    "        # end = time.time()\n",
    "        # print(f\"product {product['label_group']} got {len(results)} items for {end-start} secs\")\n",
    "        # print(f'Download time: {end - end_wait}, { (end - end_wait) / len(results)} per items')\n",
    "        return results\n",
    "\n",
    "async def scraper(products, timeout=30000):\n",
    "    tasks = []\n",
    "    for product in products:\n",
    "        tasks.append(asyncio.create_task(scrape_category(product,timeout=timeout)))\n",
    "    result = await asyncio.gather(*tasks)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_error_list = [58, 71, 103, 104, 116, 131, 156,178, 247, 253 , 277, 368, 412, 422, 424, 425, 461, 463, 475, 476, 481, 528, 541, 555, 577, 589, 624, 684, 691, 733, 759, 787, 789, 838, 843, 847, 871, 878, 879, 904, 919, 965, 980, 987, 995, 1021, 1029, 1030, 1039, 1104, 1178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_many_threads(products,result=[],start=0,end=10088,max_threads=2,max_timeout=60000):\n",
    "    first_start = start\n",
    "    prev_start = 0\n",
    "    ok_count = 0\n",
    "    error_list = []\n",
    "    error_count = 0\n",
    "    threads = min_threads = 1\n",
    "    timeout = min_timeout = 30000\n",
    "    while start < end - 1:\n",
    "        try:\n",
    "            for i in tqdm(range(start,end,threads)):\n",
    "                prev_start = start\n",
    "                if i + threads < end -1:\n",
    "                    res= await scraper(products[i:i+threads],timeout=timeout)\n",
    "                else:\n",
    "                    res= await scraper(products[i:], timeout=timeout)\n",
    "                result = result +  [x for y in res for x in y if type(x) is dict]\n",
    "                start = i + threads\n",
    "                ok_count += 1\n",
    "                if start % 2 == 0 or start % 3 == 0 or start % 5 == 0:\n",
    "                    test = pd.DataFrame.from_dict(result)\n",
    "                    test.to_csv(base_path + '/temp/scrap_temp_s_'+ str(first_start)+ \"_\" + str(start) + '.csv')\n",
    "                if ok_count > 0 and ok_count % 25 == 0:\n",
    "                    error_count = 0\n",
    "                    timeout = min(max_timeout,timeout * 2)\n",
    "                    threads = min(max_threads, threads * 2)\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            test = pd.DataFrame.from_dict(result)\n",
    "            test.to_csv(base_path + '/temp/scrap_temp_s_'+ str(first_start)+ \"_\" + str(start) + '.csv')\n",
    "            print(start)\n",
    "            print(error_list)\n",
    "            return result, error_list\n",
    "        except:\n",
    "            if prev_start == start:\n",
    "                ok_count = 0\n",
    "                error_count += 1\n",
    "            print(f'start = {start} threads = {threads}, timeout = {timeout}, error= {error_count}')\n",
    "            if error_count >= 3 or (timeout == max_timeout and threads == min_threads):\n",
    "                error_count = 0\n",
    "                error_list.append(start)\n",
    "                start = start + 1\n",
    "                timeout = min_timeout\n",
    "                threads = min_threads\n",
    "                print(error_list)\n",
    "                continue\n",
    "            if  error_count >= 2 or timeout == max_timeout:\n",
    "                threads = max (threads // 2, min_threads)\n",
    "                continue\n",
    "            if timeout < max_timeout:\n",
    "                timeout = min(timeout * 1.5, max_timeout)\n",
    "                continue\n",
    "\n",
    "    test = pd.DataFrame.from_dict(result)\n",
    "    test.to_csv(base_path + '/scrap.csv')\n",
    "    return result, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, error_list = await scrape_many_threads(products,start=1817,max_threads=12,max_timeout=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def scrape_many(products,result=[],start=0,end=10088):\n",
    "#     prev_start = 0\n",
    "#     error_count = 0\n",
    "#     error_list = []\n",
    "#     def_timeout  = timeout = 30000\n",
    "#     while start < end - 1:\n",
    "#         try:\n",
    "#             for i in tqdm(range(start,end)):\n",
    "#                 prev_start = start\n",
    "#                 res = await scrape_category(products[i],timeout)\n",
    "#                 result = result +  [x for x in res if type(x) is dict ]\n",
    "#                 start = i + 1\n",
    "#                 if i % 10 == 0 or i % 3 == 0 :\n",
    "#                     test = pd.DataFrame.from_dict(result)\n",
    "#                     test.to_csv(base_path + '/temp/scrap_temp_s_' + str(start) + \"_\" + str(time.time()) + '.csv')\n",
    "#         except KeyboardInterrupt :\n",
    "#             test = pd.DataFrame.from_dict(result)\n",
    "#             test.to_csv(base_path + '/temp/scrap_temp_s_' + str(start) + \"_\" + str(time.time()) + '.csv')\n",
    "#             print(start)\n",
    "#             print(error_list)\n",
    "#             return result, error_list\n",
    "#         except :\n",
    "#             if prev_start == start:\n",
    "#                 error_count += 1\n",
    "#             if timeout < 60000:\n",
    "#                 timeout = min(timeout * 1.5, 60000)\n",
    "#             if error_count > 3 or timeout == 60000:\n",
    "#                 error_count = 0\n",
    "#                 error_list.append(start)\n",
    "#                 start = start + 1\n",
    "#                 timeout = def_timeout\n",
    "#             continue\n",
    "#     test = pd.DataFrame.from_dict(result)\n",
    "#     test.to_csv(base_path + '/scrap.csv')\n",
    "#     return result, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, error_list = await scrape_many(products,start=272)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, error_list = await scrape_many(products,start=272)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv(base_path + '/temp/scrap_temp_s_0_160.csv')\n",
    "data_2 = pd.read_csv(base_path + '/temp/scrap_temp_s_161_276.csv')\n",
    "data_3 = pd.read_csv(base_path + '/temp/scrap_temp_s_277_1816.csv')\n",
    "data_4 = pd.read_csv(base_path + '/temp/scrap_temp_s_1817_4045.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['image'] = data_1['image'].str.removeprefix(\"../data/scrap_image/\")\n",
    "data_2['image'] = data_2['image'].str.removeprefix(\"../data/scrap_image/\")\n",
    "data_3['image'] = data_3['image'].str.removeprefix(\"../data/scrap_image/\")\n",
    "data_4['image'] = data_4['image'].str.removeprefix(\"../data/scrap_image/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([data_1,data_2,data_3,data_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['check'] = base_path + '/train_images/scrape_image/' + result['image']  \n",
    "result = result[result['check'].map(os.path.exists)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[['image','title','label_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['title'].replace(\"[^0-9a-zA-Z ,.']+\",' ',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(base_path + '/combined_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['image'] = \"scrape_image/\" + result['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train = train_df[['image','title','label_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_w_train = pd.concat([concat_train, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_w_train.to_csv(base_path + '/train_appended.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = combined_w_train.groupby(['label_group']).filter(lambda x: x['title'].size >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_2 = combined_w_train.groupby(['label_group']).filter(lambda x: x['title'].size < 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "regroup = pd.concat([group_1, group_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regroup.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regroup[['image','title','label_group']].to_csv(base_path + '/appended_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_embedding = regroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_embedding['temp'] = for_embedding['image'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_embedding = for_embedding.groupby(['temp']).filter(lambda x: x['title'].size < 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_embedding.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_embedding = for_embedding[['image','title','label_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_embedding.to_csv(base_path + '/appended_embedding.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
