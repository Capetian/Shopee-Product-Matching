{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from playwright.async_api import async_playwright\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import aiohttp        \n",
    "import aiofiles\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECTING WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data'\n",
    "# base_path = 'data'\n",
    "train_df = pd.read_csv(base_path + '/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = train_df.groupby(['label_group']).filter(lambda x: x['title'].size < 6)\n",
    "labels = groups['label_group'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups['title'] = [ s.encode('latin1').decode('unicode-escape').encode('latin1').decode('utf-8') for s in groups['title']]\n",
    "groups['title'] = groups.title.str.lower()\n",
    "groups['title'] = [re.sub('[^0-9a-zA-Z ,.]+', ' ', x) for x in groups['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups['len_words'] = [len(re.sub('[^0-9a-zA-Z ,.]+', ' ', x).split()) for x in groups['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = groups.groupby('label_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "for id in labels:\n",
    "    product = {}\n",
    "    curr = grouped.get_group(id)\n",
    "    product['label_group'] = id\n",
    "    base = r'^{}'\n",
    "    expr = '(?=.*{})'\n",
    "    v = TfidfVectorizer()\n",
    "    x = v.fit_transform(curr['title'])\n",
    "    y = v.transform([' '.join(curr[\"title\"])])\n",
    "    feature_array = v.get_feature_names_out()\n",
    "    if feature_array.shape[0] > 5:\n",
    "        tfidf_sorting = np.argsort(y.toarray()).flatten()[::-1]\n",
    "        top_n = feature_array[tfidf_sorting][:5].tolist()\n",
    "        product['query'] = ('https://shopee.co.id/search?keyword=' + \" \".join(top_n)).replace(\" \", \"%20\")\n",
    "        \n",
    "        product['words'] = base.format(''.join(expr.format(w) for w in top_n))\n",
    "    else:\n",
    "        product['query'] = ('https://shopee.co.id/search?keyword=' + curr['title'].iloc[0] ).replace(\" \", \"%20\") \n",
    "       \n",
    "        product['words'] =  base.format(''.join(expr.format(w) for w in feature_array.tolist()))\n",
    "    products.append(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_img(url,title,label_group):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as resp:\n",
    "            if resp.status == 200:\n",
    "                # start = time.time()\n",
    "                f = await aiofiles.open(title, mode='wb+')\n",
    "                await f.write(await resp.read())\n",
    "                await f.close()\n",
    "                # end = time.time()\n",
    "                # print(f\"{title} at {label_group} downloaded for {end-start} secs\")\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "\n",
    "async def get_info(item,label_group,words):\n",
    "    res = dict()\n",
    "    title = await item.get_attribute('alt')\n",
    "    matched = (re.sub('[^0-9a-zA-Z ,]+', ' ', title)).lower()\n",
    "    if re.match(words, matched):\n",
    "        link = await item.get_attribute('src')\n",
    "        url = link[:-3]\n",
    "        name = re.sub('[^0-9a-zA-Z ,]+', ' ', title)\n",
    "        name = \" \".join(name.split()[:6])\n",
    "        name = base_path + '/scrap_image/' + name + \"_\" + str(np.random.randint(0, 60)) + \"_\" + str(label_group)  + '.jpg'\n",
    "        name = name.replace(' ', \"_\")\n",
    "        down = await download_img(url,name,label_group)\n",
    "        if down:\n",
    "            res['url'] = url\n",
    "            res['image'] = name\n",
    "            res['title'] = title\n",
    "            res['label_group'] = label_group\n",
    "            return res\n",
    "        else:\n",
    "            res['url'] = url\n",
    "            res['image'] = name\n",
    "            res['title'] = title\n",
    "            res['label_group'] = label_group\n",
    "            return (res)\n",
    "\n",
    "async def scrape_category(product,timeout=30000):\n",
    "    async with async_playwright() as pw:\n",
    "        # start = time.time()\n",
    "        browser = await pw.chromium.launch( headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(product['query'])\n",
    "        # start_wait = time.time()\n",
    "        res = await page.wait_for_selector('.shopee-search-item-result', timeout=timeout)\n",
    "        end_wait = time.time()\n",
    "        # print(f\"product {product['label_group']} waiting page for {end_wait-start_wait} secs\")\n",
    "        for i in range(3):\n",
    "            await page.wait_for_selector('._7DTxhh.vc8g9F')\n",
    "            await page.keyboard.down('End')\n",
    "        selector = '._7DTxhh.vc8g9F'\n",
    "        all_items = await res.query_selector_all(selector)\n",
    "        tasks = []\n",
    "        for item in all_items:\n",
    "            tasks.append(asyncio.create_task(get_info(item,product['label_group'],product['words'])))\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        await browser.close()\n",
    "        # end = time.time()\n",
    "        # print(f\"product {product['label_group']} got {len(results)} items for {end-start} secs\")\n",
    "        # print(f'Download time: {end - end_wait}, { (end - end_wait) / len(results)} per items')\n",
    "        return results\n",
    "\n",
    "async def scraper(products, timeout=30000):\n",
    "    tasks = []\n",
    "    for product in products:\n",
    "        tasks.append(asyncio.create_task(scrape_category(product,timeout=timeout)))\n",
    "    result = await asyncio.gather(*tasks)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def test_scrape(threads, timeout=30000):\n",
    "#     start = time.time()\n",
    "#     result = []\n",
    "#     for i in range(0,21,threads):\n",
    "#         print(f\"CURR is {i}\")\n",
    "#         if i + threads < 21 -1:\n",
    "#             res= await scraper(products[i:i+threads],timeout=timeout)\n",
    "#         else:\n",
    "#             res= await scraper(products[i:], timeout=timeout)\n",
    "#         result = result + res\n",
    "#     end = time.time()\n",
    "#     print(end - start)\n",
    "#     print(len(result))\n",
    "#     return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await test_scrape(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_error_list = [58, 71, 103, 104, 116, 131, 156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_many_threads(products,result=[],start=0,end=10088,max_threads=2,max_timeout=60000):\n",
    "    first_start = start\n",
    "    prev_start = 0\n",
    "    ok_count = 0\n",
    "    error_list = []\n",
    "    error_count = 0\n",
    "    threads = min_threads = 1\n",
    "    timeout = min_timeout = 30000\n",
    "    while start < end - 1:\n",
    "        try:\n",
    "            for i in tqdm(range(start,end,threads)):\n",
    "                prev_start = start\n",
    "                if i + threads < end -1:\n",
    "                    res= await scraper(products[i:i+threads],timeout=timeout)\n",
    "                else:\n",
    "                    res= await scraper(products[i:], timeout=timeout)\n",
    "                result = result +  [x for y in res for x in y if type(x) is dict]\n",
    "                start = i + threads\n",
    "                ok_count += 1\n",
    "                if start % 2 == 0 or start % 3 == 0 or start % 5 == 0:\n",
    "                    test = pd.DataFrame.from_dict(result)\n",
    "                    test.to_csv(base_path + '/temp/scrap_temp_s_'+ str(first_start)+ \"_\" + str(start) + '.csv')\n",
    "                if ok_count > 0 and ok_count % 25 == 0:\n",
    "                    error_count = 0\n",
    "                    timeout = min(max_timeout,timeout * 2)\n",
    "                    threads = min(max_threads, threads * 2)\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            test = pd.DataFrame.from_dict(result)\n",
    "            test.to_csv(base_path + '/temp/scrap_temp_s_'+ str(first_start)+ \"_\" + str(start) + '.csv')\n",
    "            print(start)\n",
    "            print(error_list)\n",
    "            return result, error_list\n",
    "        except:\n",
    "            if prev_start == start:\n",
    "                ok_count = 0\n",
    "                error_count += 1\n",
    "            print(f'start = {start} threads = {threads}, timeout = {timeout}, error= {error_count}')\n",
    "            if error_count >= 3 or (timeout == max_timeout and threads == min_threads):\n",
    "                error_count = 0\n",
    "                error_list.append(start)\n",
    "                start = start + 1\n",
    "                timeout = min_timeout\n",
    "                threads = min_threads\n",
    "                print(error_list)\n",
    "                continue\n",
    "            if  error_count >= 2 or timeout == max_timeout:\n",
    "                threads = max (threads // 2, min_threads)\n",
    "                continue\n",
    "            if timeout < max_timeout:\n",
    "                timeout = min(timeout * 1.5, max_timeout)\n",
    "                continue\n",
    "\n",
    "    test = pd.DataFrame.from_dict(result)\n",
    "    test.to_csv(base_path + '/scrap.csv')\n",
    "    return result, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026753902435302734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 9927,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa555c08ebcf42ba8612e9d99f8e1872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start = 161 threads = 1, timeout = 30000, error= 1\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02091193199157715,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 9927,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e2bad2bcc148078cbb8a5ef6f2b3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results, error_list = await scrape_many_threads(products,start=161,max_threads=12,max_timeout=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def scrape_many(products,result=[],start=0,end=10088):\n",
    "#     prev_start = 0\n",
    "#     error_count = 0\n",
    "#     error_list = []\n",
    "#     def_timeout  = timeout = 30000\n",
    "#     while start < end - 1:\n",
    "#         try:\n",
    "#             for i in tqdm(range(start,end)):\n",
    "#                 prev_start = start\n",
    "#                 res = await scrape_category(products[i],timeout)\n",
    "#                 result = result +  [x for x in res if type(x) is dict ]\n",
    "#                 start = i + 1\n",
    "#                 if i % 10 == 0 or i % 3 == 0 :\n",
    "#                     test = pd.DataFrame.from_dict(result)\n",
    "#                     test.to_csv(base_path + '/temp/scrap_temp_s_' + str(start) + \"_\" + str(time.time()) + '.csv')\n",
    "#         except KeyboardInterrupt :\n",
    "#             test = pd.DataFrame.from_dict(result)\n",
    "#             test.to_csv(base_path + '/temp/scrap_temp_s_' + str(start) + \"_\" + str(time.time()) + '.csv')\n",
    "#             print(start)\n",
    "#             print(error_list)\n",
    "#             return result, error_list\n",
    "#         except :\n",
    "#             if prev_start == start:\n",
    "#                 error_count += 1\n",
    "#             if timeout < 60000:\n",
    "#                 timeout = min(timeout * 1.5, 60000)\n",
    "#             if error_count > 3 or timeout == 60000:\n",
    "#                 error_count = 0\n",
    "#                 error_list.append(start)\n",
    "#                 start = start + 1\n",
    "#                 timeout = def_timeout\n",
    "#             continue\n",
    "#     test = pd.DataFrame.from_dict(result)\n",
    "#     test.to_csv(base_path + '/scrap.csv')\n",
    "#     return result, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, error_list = await scrape_many(products,start=272)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, error_list = await scrape_many(products,start=272)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43merror_list\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error_list' is not defined"
     ]
    }
   ],
   "source": [
    "print(error_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
